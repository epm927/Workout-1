---
title: "Workout 1"
author: "Ethan M."
date: "October 2, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
```

## 1) File Structure

Following the commands outlined in the assignment I created the necessary folders: (I know the document says not to use `eval = FALSE` but in order to display the commands I used in terminal I have to)
```{r, eval = FALSE}
cd stat133-hws-fall18
mkdir workout1
cd workout1
touch README.md
mkdir data
mkdir code
mkdir output
mkdir report
```

After a series of commands in the terminal (initializing, configuring, etc.), I have pushed the necessary files into my [repository](https://github.com/epm927/Workout-1.git). 

### Downloading the Data:

Using the `curl -O` command (in the terminal, inside the `data/` folder), I downloaded the data.


\ 

## 2) Create a **`README`** file

I added some text to the README file, outlining the goals of this project as well as dating the project. 

\ 

## 3) Create a Data Dictionary

I used the command `touch nba2018-dictionary.md` (while inside the `data/` directory) to create a blank data dictionary. Using `open`, I opened the file (using LaTeX, by default) and edited the dictionary. To examine the data, I will import it here into `R`, and perform exploration:

```{r}
dat <- read.csv("../data/nba2018.csv")
length(dat[,1])
```

Looks like there are 477 lines of data. After adding this information to the dictionary, I added, committed, and pushed everything onto the repository again.

\ 

Now I switch to an `R` script. See you on the flipside!

\ 

## 5) Ranking of Teams

Let's now import our nba2018-teams.csv file

```{r}
dat <- read.csv("../data/nba2018-teams.csv")
length(dat[,1])
```

### Basic Rankings

#### Ranked by Salary:

First we create `by.salary`, a variable which has the teams odered in decreasing order of salary:
```{r}
by.salary <- arrange(dat, desc(salary))
df <- data.frame(by.salary)
```

Now we plot:
```{r salary_ranking}
ggplot(df, aes(x= reorder(team, salary), y = salary)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(
    title = "NBA Teams ranked by Salary",
    y = "Salary (in millions)",
    x = "Team"
  ) + 
  geom_hline(yintercept = mean(df$salary), col = "red", size = 2, alpha = 0.5)
```

\ 

#### Ranked by Total Points:
```{r}
by.points <- arrange(dat, desc(points))
df2 <- data.frame(by.points)
```

```{r, points_ranking}
ggplot(df2, aes(x= reorder(team, points), y = points)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(
    title = "NBA Teams ranked by Total Points",
    y = "Total Points",
    x = "Team"
  ) + 
  geom_hline(yintercept = mean(df2$points), col = "red", size = 2, alpha = 0.5)
```

\ 

#### Ranked by Efficiencies
```{r}
by.eff <- arrange(dat, desc(efficiency))
df3 <- data.frame(by.eff)
```

```{r, efficiency_ranking}
ggplot(df3, aes(x= reorder(team, efficiency), y = efficiency)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(
    title = "NBA Teams ranked by Efficiency",
    y = "Efficiency",
    x = "Team"
  ) + 
  geom_hline(yintercept = mean(df3$efficiency), col = "red", size = 2, alpha = 0.5)
```

#### A new ranking: Experience-Efficiency

Let's define a new index: the "experience-efficiency" index, denoted $\varepsilon$ and defined as $$\varepsilon = \frac{\text{Efficiency} + \text{Experience}}{2} $$ I wanted to create an index that included both efficiencies and experiences- averaging these two values made the most sense.

We first then need to mutate the original data to include this new ranking:

```{r}
dat.2 <- mutate(dat, epsilon = (efficiency + experience)/2)
```

Now we can proceed as we did in previous rankings:

```{r}
by.eps <- arrange(dat.2, desc(epsilon))
df4 <- data.frame(by.eps)
```

```{r, epsilon_ranking}
ggplot(df4, aes(x= reorder(team, epsilon), y = epsilon)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(
    title = "NBA Teams ranked by Experience-Efficiency (epsilon)",
    y = "Epsilon",
    x = "Team"
  ) + 
  geom_hline(yintercept = mean(df4$epsilon), col = "red", size = 2, alpha = 0.5)
```

Note some of the differences between ranking by efficiency and ranking by $\varepsilon$; the experience-efficiency index benefitted teams with slightly lower efficiencies but higher experience points. For example, in a pure efficiency ranking `PHI` is ranked the lowes whereas under an experience-efficiency ranking `PHI` moves up in ranking to the middle. 

\ 

### Comments and Reflections:

* This was my first time working on a project with such a file structure. I found it particularl illuminating, though, and really enjoyed getting a flavor of how more complicated filestructures work (I suspect that in the "real world" one frequently encounters even more complicated structures!) 
* Yes, this was my first time using relative paths. Having many different levels enabled me to get practice with specifying relative paths, and navigating through different directories in the Terminal. As such, I feel much more confident in using them, and can definitely see their benefit!
* I have used R scripts in the past, but only for very basic codes (for example, importing data sets and producing plots, or defining functions and plotting those). 
* Initially, getting used to relative paths was challenging, but I think I got the hang of things. I also had a small bug in one of my loops (I had used `i <= i + 1` instead of `i <- i + 1`), and spent probably half an hour trying to figure out why my code just kept running but never stopping! [Finally I went to Office Hours and one of the GSI's spotted the mistake... so thank you!]
* Even though we did cover this in class, I feel like I'm getting better at using `arrange` and other `dplyr` functions. I wouldn't necessarily say they come "esay", but they are coming "easier"
* I began this workout around Monday or Tuesday, and finished it today (Thursday). Obviously I didn't work on this for the entirety of these days, however. In total, I think spent somewhere between 6 and 8 hours on this project. 
* The most time consuming part was probably that code bug I mentioned above... I also found the data preprocessing to be quite time consuming, but it ocurred to me while performing the preprrocessing that the majority of data analysis is probably devoted to getting the data to a state in which `R` can read it! 
* I found the final result quite enjoyable- having a "project" completed feels oddly satisfying.